---
title: Improve Memory Efficiency in Model Building
tags: Python
---

During machine learning model building process, insufficient memory is a common issue especially for boosting or ensemble models. In this post, some techniques which I commonly use to reduce the memory usage are shared.


### Lazy Evaluation

Let's start with a simple example of using [scikit-learn](http://scikit-learn.org/) and [pandas](http://pandas.pydata.org/) to train a Random Forest classifier.

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

target = 'target'           # target column name
feats = ['feat1', 'feat2']  # feature column names

cls = RandomForestClassifier()
cls.fit_transform(train_df[feats], train_df[target])
y_pred = cls.predict(test_df[feats])
```

The above code works well when both training and testing datasets can be fitted into memory. The insufficient memory issue appears when the computer memory is sufficient only for one dataset. A simple way to get around this issue would be loading the dataset only when it is required.

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

def build_model(filename, feats, target):
    df = pd.read_csv(filename)
    cls = RandomForestClassifier()
    cls.fit_transform(df[feats], df[target])
    return cls

target = 'target'           # target column name
feats = ['feat1', 'feat2']  # feature column names
cls = build_model('train.csv', feats, target)
test_df = pd.read_csv('test.csv')
y_pred = cls.predict(test_df[feats])
```

Wait! This isn't [Lazy Evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation)? To achieve the effect, the code can be written in a functional way. The resources are loaded only when they are required.

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

def load_feature(filename, feats):
    return pd.read_csv(filename, usecols=feats)

def load_target(filename, target):
    return pd.read_csv(filename, usecols=[target]).values

def build_model(filename, feats, target):
    cls = RandomForestClassifier()
    cls.fit_transform(load_feature(filename, feats),
                      load_target(filename, target))
    return cls

def predict(cls, filename, feats):
    return csl.predict(load_feature(filename, feats))

train_file, test_file = 'train.csv', 'test.csv'
target = 'target'           # target column name
feats = ['feat1', 'feat2']  # feature column names
y_pred = predict(build_model(train_file, feats, target), test_file, feats)
```


### Sparse Matrix

[One-hot encoding](https://en.wikipedia.org/wiki/One-hot) is a popular technique which is used to preprocess categorical features. When combining numerical features with one-hot encoded features, it is suggested to store the output in sparse matrix format to reduce the memory usage. The effect of reduction memory usage is more significant when the number of categories is very high.

```python
import numpy as np
import pandas as pd
import scipy.sparse as sparse
from sklearn.feature_extraction.text.CountVectorizer

sp_feats = ['cat1']                 # categorical feature name
dn_feats = ['num1', 'num2']         # numerical feature name
df = pd.read_csv('train.csv')
vec = CountVectorizer()
sp_matrix = vec.fit_transform(df[feats].to_dict(orient='record'))   # sparse
dn_matrix = df[dn_feats].values

# dense matrix hstack -- memory inefficient
X = np.hstack((sp_matrix.toarray(), dn_matrix))

# sparse matrix hstack -- memory efficient
X = sparse.hstack((sp_matrix, sparse.csr_matrix(dn_matrix)), format='csr')
```


### Reduce Precision

Storing matrix in lower precision consumes less memory. This approach is viable only if it does not result loss of precision. When the feature matrix contains value which exceeds the maximum value of 32-bit float, this approach is not longer applicable.

```python
import numpy as np
import pandas as pd

# reduce the precision from 64-bit to 32-bit
def load_matrix(filename):
    return pd.read_csv(filename).values.astype(np.float32)
```
